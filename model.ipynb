{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raichu/Desktop/projects/NLP_task/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting device to 'cuda' if available else 'cpu' for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "downloading dataset and the tokenizer to tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting the data from the downloaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28711\n"
     ]
    }
   ],
   "source": [
    "new_dataset = copy.deepcopy(dataset)\n",
    "\n",
    "train_subset = dataset[\"train\"].select(range(int(len(dataset[\"train\"]) * 0.1)))\n",
    "\n",
    "print(len(train_subset))\n",
    "\n",
    "new_dataset[\"train\"] = train_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing the data before feeding it to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Truncate or pad inputs and outputs\n",
    "    inputs = [\"summarize: \" + article for article in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Adjust labels to ignore padding during loss computation\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = new_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing a dataloader to load data in batches in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"validation\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder part of the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # (batch_size, seq_len)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)  # outputs: (batch_size, seq_len, hidden_dim)\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attentioni mechanism to focus on the important parts of the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Repeat decoder hidden state for each time step\n",
    "        decoder_hidden = decoder_hidden[0]\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # Calculate attention scores\n",
    "        energy = torch.tanh(self.attn(torch.cat((decoder_hidden, encoder_outputs), dim=2)))  # (batch_size, seq_len, hidden_dim)\n",
    "        attention = torch.sum(self.v * energy, dim=2)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.softmax(attention, dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Compute context vector\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (batch_size, 1, hidden_dim)\n",
    "        context_vector = context_vector.squeeze(1)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder part of the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.attention = attention\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        self.rnn = nn.LSTM(emb_dim + hidden_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Linear layer to output probabilities over the vocabulary\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input, decoder_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).unsqueeze(1)  # (batch_size, 1, emb_dim)\n",
    "        \n",
    "        # Get context vector from attention\n",
    "        context_vector, _ = self.attention(decoder_hidden[0], encoder_outputs)  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Concatenate context vector with embedded input token\n",
    "        rnn_input = torch.cat((embedded, context_vector.unsqueeze(1)), dim=2)  # (batch_size, 1, emb_dim + hidden_dim)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, cell) = self.rnn(rnn_input, decoder_hidden)\n",
    "        \n",
    "        # Predict the next word\n",
    "        prediction = self.fc_out(output.squeeze(1))  # (batch_size, output_dim)\n",
    "        \n",
    "        return prediction, (hidden, cell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting all together to form the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, pad_idx):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        trg_len = trg.size(1)\n",
    "        batch_size = src.size(0)\n",
    "        \n",
    "        # Initialize tensors to hold the outputs and hidden states\n",
    "        outputs = torch.zeros(batch_size, trg_len, self.decoder.output_dim).to(self.device)\n",
    "        \n",
    "        # Encoder outputs and hidden state\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(src)\n",
    "        \n",
    "        # First input to the decoder is the <sos> token (start of sequence)\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # Forward pass through the decoder\n",
    "            output, (hidden, cell) = self.decoder(input, (hidden, cell), encoder_outputs)\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Get the highest probability token and decide whether to use teacher forcing\n",
    "            top1 = output.argmax(1)  \n",
    "            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "        \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some parameters for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(tokenizer.vocab)\n",
    "OUTPUT_DIM = len(tokenizer.vocab)\n",
    "EMB_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "attention = Attention(HIDDEN_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT, attention)\n",
    "model = Seq2Seq(encoder, decoder, device=device, pad_idx=PAD_IDX).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def calculate_loss(pred, target):\n",
    "    # Compute the loss ignoring the padding token\n",
    "    loss = F.cross_entropy(pred.view(-1, pred.shape[-1]), target.view(-1), ignore_index=PAD_IDX)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traing loop for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, device, num_epochs=5):\n",
    "    for epoch in range(num_epochs):  # Iterate over epochs\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        # Training phase with progress bar\n",
    "        with tqdm(train_loader, unit=\"batch\", desc=f\"Epoch {epoch+1}/{num_epochs}\") as tepoch:\n",
    "            for batch in tepoch:\n",
    "                src = batch[\"input_ids\"].to(device)\n",
    "                trg = batch[\"labels\"].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(src, trg)\n",
    "                \n",
    "                loss = calculate_loss(output, trg)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_train_loss += loss.item()\n",
    "                tepoch.set_postfix(train_loss=total_train_loss / (tepoch.n + 1))  # Display loss in progress bar\n",
    "        \n",
    "        # Training loss for the epoch\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            with tqdm(val_loader, unit=\"batch\", desc=f\"Validation {epoch+1}/{num_epochs}\") as vepoch:\n",
    "                for batch in vepoch:\n",
    "                    src = batch[\"input_ids\"].to(device)\n",
    "                    trg = batch[\"labels\"].to(device)\n",
    "\n",
    "                    output = model(src, trg)\n",
    "                    \n",
    "                    loss = calculate_loss(output, trg)\n",
    "                    total_val_loss += loss.item()\n",
    "                    vepoch.set_postfix(val_loss=total_val_loss / (vepoch.n + 1))  # Display loss in progress bar\n",
    "        \n",
    "        # Validation loss for the epoch\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 2393/2393 [48:05<00:00,  1.21s/batch, train_loss=6.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 6.7205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation 1/5: 100%|██████████| 1671/1671 [06:30<00:00,  4.28batch/s, val_loss=6.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation Loss: 6.6767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 2393/2393 [48:09<00:00,  1.21s/batch, train_loss=6.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training Loss: 6.2505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation 2/5: 100%|██████████| 1671/1671 [06:29<00:00,  4.29batch/s, val_loss=6.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation Loss: 6.4814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 2393/2393 [48:06<00:00,  1.21s/batch, train_loss=6]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training Loss: 6.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation 3/5: 100%|██████████| 1671/1671 [06:28<00:00,  4.30batch/s, val_loss=6.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation Loss: 6.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 2393/2393 [47:52<00:00,  1.20s/batch, train_loss=5.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training Loss: 5.8096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation 4/5: 100%|██████████| 1671/1671 [06:28<00:00,  4.30batch/s, val_loss=6.33]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation Loss: 6.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 2393/2393 [47:53<00:00,  1.20s/batch, train_loss=5.67]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training Loss: 5.6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation 5/5: 100%|██████████| 1671/1671 [06:28<00:00,  4.30batch/s, val_loss=6.3] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Validation Loss: 6.2993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing data to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> Token: None, Index: 0\n",
      "<eos> Token: </s>, Index: 1\n"
     ]
    }
   ],
   "source": [
    "test_data = next(iter(test_loader))\n",
    "\n",
    "input_data = test_data[\"input_ids\"].to(device)\n",
    "input_labels = test_data[\"labels\"].to(device)\n",
    "\n",
    "# The special tokens for T5\n",
    "sos_token = tokenizer.cls_token  # T5 does not use <sos>; instead, <pad> is used to start decoding\n",
    "eos_token = tokenizer.eos_token  # End of sequence token for T5\n",
    "\n",
    "# Retrieve the indices\n",
    "sos_idx = tokenizer.pad_token_id  # T5 uses <pad> (padding token) as the starting token\n",
    "eos_idx = tokenizer.eos_token_id  # Index of the <eos> token\n",
    "\n",
    "print(f\"<sos> Token: {sos_token}, Index: {sos_idx}\")\n",
    "print(f\"<eos> Token: {eos_token}, Index: {eos_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "making prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, src_tensor, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode the input sentence\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(src_tensor)\n",
    "\n",
    "        # Start with the <sos> token\n",
    "        trg_indexes = [sos_idx]  # Replace with your <sos> token index\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            trg_tensor = torch.tensor([trg_indexes[-1]], dtype=torch.long).to(device)\n",
    "\n",
    "            # Decode the next token\n",
    "            output, (hidden, cell) = model.decoder(trg_tensor, (hidden, cell), encoder_outputs)\n",
    "\n",
    "            # Get the token with the highest probability\n",
    "            pred_token = output.argmax(1).item()\n",
    "            trg_indexes.append(pred_token)\n",
    "\n",
    "            # Stop if the <eos> token is generated\n",
    "            if pred_token == eos_idx:  # Replace with your <eos> token index\n",
    "                break\n",
    "\n",
    "        return trg_indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicting from the input test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: summarize: (CNN)Governments around the world are using the threat of terrorism -- real or perceived -- to advance executions, Amnesty International alleges in its annual report on the death penalty. \"The dark trend of governments using the death penalty in a futile attempt to tackle real or imaginary threats to state security and public safety was stark last year,\" said Salil Shetty, Amnesty's Secretary General in a release. \"It is shameful that so many states around the world are essentially playing with people's lives -- putting people to death for 'terrorism' or to quell internal instability on the ill-conceived premise of deterrence.\" The report, \"Death Sentences and Executions 2014,\" cites the example of Pakistan lifting a six-year moratorium on the execution of civilians following the horrific attack on a school in Peshawar in December. China is also mentioned, as having used the death penalty as a tool in its \"Strike Hard\" campaign against terrorism in the restive far-western province of Xinjiang. The annual report catalogs the use of state-sanctioned killing as a punitive measure across the globe, and this year's edition contains some mixed findings. On one hand, the number of executions worldwide has gone down by almost 22% on the previous year. At least 607 people were executed around the world in 2014, compared to 778 in 2013. Amnesty's figures do not include statistics on executions carried out in China, where information on the practice is regarded as a state secret. Belarus and Vietnam, too, do not release data on death penalty cases. \"The long-term trend is definitely positive -- we are seeing a decrease in the number of executions (worldwide),\" Audrey Gaughran, Amnesty's Director of Global Issues, told CNN. \"A number of countries are closer to abolition, and there are some signs that some countries will be abolitionist by 2015. (There are) signals of a world that is nearing abolition.\" While the report notes some encouraging signs, it also highlights a marked increase in the number of people sentenced to death in 2014. At least 2,466 people globally are confirmed to have been handed the sentence last year, an increase of</s>\n",
      "Predicted Sentence: <pad>asss a  of the  ....  .   . ... . ..... \n"
     ]
    }
   ],
   "source": [
    "prediction = predict(model, input_data[6].unsqueeze(0))\n",
    "\n",
    "input_sentence = tokenizer.decode(input_data[6])\n",
    "predicted_sentence = tokenizer.decode(prediction)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Predicted Sentence:\", predicted_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
